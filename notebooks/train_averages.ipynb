{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "from src.error_measures import collect_stats, print_average_stats\n",
    "from src.models import MultiLayerANN, SynapticSNN, LeakySNN, DoubleLeakySNN\n",
    "from src.train_model import training_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cpu')"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def multiple_trainings(dataset, trials=3, hidden=3000):\n",
    "    err_model1 = [0., 0., 0., 0., 0., 0.]\n",
    "    err_model2 = [0., 0., 0., 0., 0., 0.]\n",
    "    err_model3 = [0., 0., 0., 0., 0., 0.]\n",
    "    err_model4 = [0., 0., 0., 0., 0., 0.]\n",
    "\n",
    "    for i in range(trials):\n",
    "        # Prepare data loaders.\n",
    "        train_dataset, test_dataset = torch.utils.data.random_split(dataset, (round(0.8 * len(dataset)), round(0.2 * len(dataset))))\n",
    "        train_loader = data.DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True)\n",
    "        test_loader = data.DataLoader(test_dataset, batch_size=256, shuffle=False, drop_last=False)\n",
    "\n",
    "        # Prepare models.\n",
    "        model1 = MultiLayerANN(num_inputs=8, num_hidden=hidden, num_outputs=1)\n",
    "        model2 = LeakySNN(num_inputs=8, num_hidden=hidden, num_outputs=1, num_steps=25)\n",
    "        model3 = SynapticSNN(num_inputs=8, num_hidden=hidden, num_outputs=1, num_steps=50)\n",
    "        model4 = DoubleLeakySNN(num_inputs=8, num_hidden=hidden, num_outputs=1, num_steps=20)\n",
    "\n",
    "        # Train models.\n",
    "        print(f'------------------------------------\\nTraining {i+1}')\n",
    "        training_loop(model1, train_loader, device, num_epochs=120) #90\n",
    "        training_loop(model2, train_loader, device, num_epochs=62) #50\n",
    "        training_loop(model3, train_loader, device, num_epochs=30) #25\n",
    "        training_loop(model4, train_loader, device, num_epochs=37) #35\n",
    "\n",
    "        # Gather results.\n",
    "        collect_stats(model1, err_model1, test_loader, device)\n",
    "        collect_stats(model2, err_model2, test_loader, device)\n",
    "        collect_stats(model3, err_model3, test_loader, device)\n",
    "        collect_stats(model4, err_model4, test_loader, device)\n",
    "\n",
    "        # Print average results.\n",
    "    print('------Simple ANN ------')\n",
    "    print_average_stats(err_model1, trials)\n",
    "    print('------LIF SNN ------')\n",
    "    print_average_stats(err_model2, trials)\n",
    "    print('------Synaptic SNN ------')\n",
    "    print_average_stats(err_model3, trials)\n",
    "    print('------Double LIF SNN ------')\n",
    "    print_average_stats(err_model4, trials)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data 6h"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "data06 = pd.read_csv(\"../data/processed/target06.csv\")\n",
    "data06.drop(columns=[\"weekday\"], inplace=True)\n",
    "all_data = data.TensorDataset(torch.from_numpy((data06.values[:,:-1] - data06.values[:,:-1].min(0)) / data06.values[:,:-1].ptp(0)).float(),\n",
    "                              torch.from_numpy(data06.values[:,-1]).float())  # with normalization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "Training 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 4/62 [00:49<12:02, 12.46s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [10], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmultiple_trainings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mall_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrials\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3000\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn [8], line 22\u001B[0m, in \u001B[0;36mmultiple_trainings\u001B[1;34m(dataset, trials, hidden)\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m------------------------------------\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mTraining \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m#training_loop(model1, train_loader, device, num_epochs=120) #90\u001B[39;00m\n\u001B[1;32m---> 22\u001B[0m \u001B[43mtraining_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m62\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m#50\u001B[39;00m\n\u001B[0;32m     23\u001B[0m training_loop(model3, train_loader, device, num_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m30\u001B[39m) \u001B[38;5;66;03m#25\u001B[39;00m\n\u001B[0;32m     24\u001B[0m training_loop(model4, train_loader, device, num_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m37\u001B[39m) \u001B[38;5;66;03m#35\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\Moje dokumenty\\Studia\\Praca Inżynierska\\snn-pollution\\src\\train_model.py:27\u001B[0m, in \u001B[0;36mtraining_loop\u001B[1;34m(model, train_loader, device, num_epochs, lr)\u001B[0m\n\u001B[0;32m     25\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss_fun(preds, targets)\n\u001B[0;32m     26\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 27\u001B[0m     \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Gradient calculation\u001B[39;00m\n\u001B[0;32m     28\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()  \u001B[38;5;66;03m# Weight update\u001B[39;00m\n\u001B[0;32m     29\u001B[0m scheduler\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[1;32m~\\Documents\\Moje dokumenty\\Studia\\Praca Inżynierska\\snn-pollution\\venv\\lib\\site-packages\\torch\\_tensor.py:396\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    387\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    388\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    389\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    390\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    394\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[0;32m    395\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[1;32m--> 396\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Moje dokumenty\\Studia\\Praca Inżynierska\\snn-pollution\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    168\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    170\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 173\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Moje dokumenty\\Studia\\Praca Inżynierska\\snn-pollution\\venv\\lib\\site-packages\\torch\\autograd\\function.py:243\u001B[0m, in \u001B[0;36mBackwardCFunction.apply\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m    242\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mBackwardCFunction\u001B[39;00m(_C\u001B[38;5;241m.\u001B[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001B[1;32m--> 243\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs):\n\u001B[0;32m    244\u001B[0m         \u001B[38;5;66;03m# _forward_cls is defined by derived class\u001B[39;00m\n\u001B[0;32m    245\u001B[0m         \u001B[38;5;66;03m# The user should define either backward or vjp but never both.\u001B[39;00m\n\u001B[0;32m    246\u001B[0m         backward_fn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_cls\u001B[38;5;241m.\u001B[39mbackward  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[0;32m    247\u001B[0m         vjp_fn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_cls\u001B[38;5;241m.\u001B[39mvjp  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "multiple_trainings(all_data, trials=1, hidden=3000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data 12h"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data12 = pd.read_csv(\"../data/processed/target12.csv\")\n",
    "data12.drop(columns=[\"weekday\"], inplace=True)\n",
    "all_data = data.TensorDataset(torch.from_numpy((data12.values[:,:-1] - data12.values[:,:-1].min(0)) / data12.values[:,:-1].ptp(0)).float(),\n",
    "                              torch.from_numpy(data12.values[:,-1]).float())  # with normalization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "multiple_trainings(all_data, trials=2, hidden=3000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data 24h"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data24 = pd.read_csv(\"../data/processed/target24.csv\")\n",
    "data24.drop(columns=[\"weekday\"], inplace=True)\n",
    "all_data = data.TensorDataset(torch.from_numpy((data24.values[:,:-1] - data24.values[:,:-1].min(0)) / data24.values[:,:-1].ptp(0)).float(),\n",
    "                              torch.from_numpy(data24.values[:,-1]).float())  # with normalization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "multiple_trainings(all_data, trials=2, hidden=3000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}